# Unified ROCm + ROS2 + Gazebo Container

## Overview

**Everything in ONE container!** üöÄ

This unified setup combines:
- ‚úÖ **ROCm GPU acceleration** (20GB VRAM)
- ‚úÖ **ROS2 Jazzy** (full desktop install)
- ‚úÖ **Gazebo Harmonic** (physics simulation)
- ‚úÖ **RL Training** (Stable-Baselines3 with GPU)

**Benefits over two-container setup:**
- üî• **Simpler** - No network sharing, no RMW sync issues
- üî• **Faster** - No network overhead between containers
- üî• **Cleaner** - One environment, one terminal
- üî• **GPU everywhere** - RL training AND Gazebo in same container

---

## One-Time Setup

### Build the Container (~10-15 minutes)

```bash
# On host machine
cd ~/pidog_ros2

# Make setup script executable (if not already)
chmod +x setup_unified_container.sh

# Build the unified container
./setup_unified_container.sh
```

**What it does:**
1. Builds Docker image with ROCm PyTorch + ROS2 + Gazebo
2. Installs all dependencies
3. Configures X11 for Gazebo GUI

---

## Daily Usage

### Start Container

```bash
# On host - allow X11 for Gazebo GUI
xhost +local:docker

# Start unified container
docker-compose --profile unified run --rm pidog-unified

# You're now inside the container with EVERYTHING ready!
```

### Inside Container: Build Workspace (First Time)

```bash
# Build ROS2 workspace
colcon build

# Source workspace
source install/setup.bash

# Verify GPU
python3 -c "import torch; print('GPU:', torch.cuda.is_available())"

# Verify ROS2
ros2 topic list

# Everything should work! ‚úì
```

---

## Complete Training Workflow (All in ONE Container!)

### Step 1: Collect Training Data

```bash
# Launch Gazebo + gait generator + data collector
ros2 launch pidog_gaits collect_data_enhanced.launch.py

# Wait for completion (~10-20 min for full dataset)
# OR collect manually with custom duration:
# ./collect_training_data.sh 90
```

### Step 2: Train Imitation Model (GPU)

```bash
# GPU-accelerated imitation learning
python3 -m pidog_gaits.pidog_gaits.train \
    --data ./training_data/gait_data_*.npz \
    --model simple_lstm \
    --epochs 200 \
    --batch_size 1024 \
    --device cuda

# Time: ~30 min with 20GB GPU
# Output: ./models/best_model.pth
```

### Step 3: RL Fine-Tuning (GPU)

```bash
# GPU-accelerated RL training with 4 parallel environments
python3 -m pidog_gaits.pidog_gaits.train_rl \
    --pretrained ./models/best_model.pth \
    --output ./models/rl \
    --timesteps 100000 \
    --envs 4 \
    --device cuda

# Time: ~30 min with 4 envs on 20GB GPU
# Output: ./models/rl/final_model.zip
```

### Step 4: Test RL Model

```bash
# Launch Gazebo with RL-trained model
ros2 launch pidog_gaits nn_demo.launch.py

# Send commands
ros2 topic pub /gait_command std_msgs/msg/String "data: 'walk_forward'" --once
```

---

## Quick Commands Reference

### Gazebo Only
```bash
ros2 launch pidog_description gazebo.launch.py
```

### Traditional Gaits Demo
```bash
ros2 launch pidog_gaits gait_demo.launch.py
ros2 topic pub /gait_command std_msgs/msg/String "data: 'trot_forward'" --once
```

### Neural Network Demo
```bash
ros2 launch pidog_gaits nn_demo.launch.py
ros2 topic pub /gait_command std_msgs/msg/String "data: 'walk_forward'" --once
```

### Rebuild Workspace (After Code Changes)
```bash
./rebuild.sh
source install/setup.bash
```

---

## Performance with 20GB GPU

| Task | Time | VRAM Usage |
|------|------|------------|
| Imitation training (batch=1024) | ~30 min | ~4 GB |
| RL training (4 envs) | ~30 min | ~10 GB |
| RL training (8 envs) | ~15 min | ~18 GB |

**Recommendation:** Use 4 parallel environments for best stability/speed balance.

---

## Troubleshooting

### "cannot open display"
```bash
# On host, before starting container:
xhost +local:docker
```

### GPU not detected
```bash
# Inside container, check:
python3 -c "import torch; print(torch.cuda.is_available())"
rocm-smi  # Should show your GPU

# If false, check host:
# docker run --rm -it --device=/dev/kfd --device=/dev/dri rocm/pytorch:latest rocm-smi
```

### Gazebo crashes
```bash
# Rebuild workspace
./rebuild.sh
source install/setup.bash

# Check GPU setup
./setup_rendering.sh  # May not be needed in unified container
```

### Code changes not applying
```bash
# Full rebuild
./rebuild.sh

# Nuclear option
./clean_all.sh
colcon build
source install/setup.bash
```

---

## Comparison: Unified vs Two-Container Setup

| Aspect | Unified (NEW) | Two-Container (OLD) |
|--------|---------------|---------------------|
| Containers | 1 | 2 |
| Setup complexity | Low | High |
| Network config | Simple (host) | Complex (container sharing) |
| RMW sync | Not needed | Required |
| GPU access | Direct | Direct |
| Gazebo location | Same container | Separate |
| RL training | Same container | Separate |
| Maintenance | Easy | Complex |
| **Recommendation** | ‚úÖ **USE THIS** | ‚ö†Ô∏è Legacy |

---

## Files

**Created:**
- `Dockerfile.rocm-ros2` - Unified container definition
- `setup_unified_container.sh` - One-time build script
- `UNIFIED_CONTAINER.md` - This guide

**Modified:**
- `docker-compose.yml` - Added `pidog-unified` service

**Existing (unchanged):**
- All ROS2 packages
- RL training scripts
- Data collection scripts

---

## Summary

**Start here for new users:**

```bash
# ONE-TIME: Build container
./setup_unified_container.sh

# DAILY: Start container
xhost +local:docker
docker-compose --profile unified run --rm pidog-unified

# Inside container: Build workspace (first time)
colcon build && source install/setup.bash

# Collect data ‚Üí Train imitation ‚Üí Train RL ‚Üí Test
# All with GPU acceleration in ONE container! üöÄ
```

**Migration from two-container setup:** Just use the unified container going forward. Your existing data and models will work as-is.
